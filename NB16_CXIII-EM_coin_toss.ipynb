{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 16: Expectation Maximization in practice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goal \n",
    "The goal of this notebook is to gain intuition for Expectation Maximization using a simple example involving coin tosses.\n",
    "\n",
    "## Overview\n",
    "\n",
    "In Section XIV, we introduce Expectation-Maximization (EM) as a practical way to perform maximum likelihood estimation (MLE) even when some of the data is hidden (i.e in the presence of latent or hidden variables). To better understand EM, in this short notebook we'll explore a very simple coin-tossing example adapted from [Do and Batzoglou, Nat. Biotechnol. (2008)](https://www.nature.com/articles/nbt1406). \n",
    "\n",
    "Suppose that we are given two coins A and B with unkown bias $\\theta_A$ and $\\theta_B$, respectively. Our goal is to estimate the bias vector $\\boldsymbol{\\theta}= (\\theta_A, \\theta_B)$ from the outcomes of the following experiment: \n",
    "\n",
    "<blockquote> \n",
    "First choose one coin at random. Then toss the selected coin 10 times independently and record the number of heads observed. Repeat this procedure 5 times.\n",
    "</blockquote>\n",
    "\n",
    "Formally, let $z_i\\in\\{A,B\\}$ be the coin selected in experiment $i$ and $x_i\\in\\{0,1,\\cdots 10\\}$ be the number heads recorded by tossing $z_i$ 10 times. Since we conduct $n=5$ such experiments, we can summarize the outcomes of these 50 tosses by two vectors: $\\boldsymbol{x}=(x_1,x_2\\cdots, x_5)$ and $\\boldsymbol{z}=(z_1,z_2,\\cdots, z_5)$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: What if we know everything?\n",
    "\n",
    " * Consider first the case where we have complete knowledge of the experiment, namely, both $\\boldsymbol{x}$ and $\\boldsymbol{z}$ are known. How would you intuitively estimate the biases of the two coins  $\\boldsymbol{\\theta}= (\\theta_A, \\theta_B)$ ?\n",
    " \n",
    "> For either coin, the ratio of the number of times it throws heads (as opposed to tails), would be my estimate of it's bias\n",
    "\n",
    " \n",
    " * What's the likelihood of observing the complete outcomes of these experiments? In other words, what is $P(\\boldsymbol{x},\\boldsymbol{z}| n,\\boldsymbol{\\theta} )$? You may assume this is a Bernoulli trial. Namely, every time coin A(B) is tossed, we have, with probability $\\theta_A$($\\theta_B$), that the outcome is heads.\n",
    " \n",
    "\n",
    "> $\\prod_i$ $(Binomial A)^{Z_i}(Binomial B)^{1- Z_i}$, where the binomials are $\\binom{10}{x_i}\\Theta_{A/B}^{x_i} (1-\\Theta_{A/B})^{10-x_i}$\n",
    " \n",
    " * What's the Maximum Likelihood Estimator (MLE)? Is this consistent with your intuition? \n",
    "\n",
    "> It is the ratio of the numbers of heads to the total throws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing MLE and EM\n",
    "\n",
    "To test your answer, let's do some numerics! We will compare the MLE estimates of biases with an Expectation Maximization procedure where we do not know ${\\bf z}$. The following code computes our best guess for the biases using MLE -- assuming we know the identity of the coin used -- and compares it estimates arrived at using an EM procedure where we have no knowledge about which coin was being tossed (though we know the same coin was tossed 10 times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial guess A: 0.8809033391213278\n",
      "Initial guess B: 0.3239088237069466\n",
      "At iteration 0, theta_A = 0.741743,  theta_B = 0.611551\n",
      "At iteration 1, theta_A = 0.721972,  theta_B = 0.675281\n",
      "At iteration 2, theta_A = 0.708837,  theta_B = 0.691143\n",
      "At iteration 3, theta_A = 0.703369,  theta_B = 0.696631\n",
      "At iteration 4, theta_A = 0.701283,  theta_B = 0.698717\n",
      "At iteration 5, theta_A = 0.700489,  theta_B = 0.699511\n",
      "E-M converges at iteration 6\n",
      "RESULT:\n",
      "E-M: theta_A = 0.700489,  theta_B = 0.699511\n",
      "MLE with complete data: theta_A = 0.725000,  theta_B = 0.600000\n",
      " \n",
      " \n",
      "Initial guess A: 0.5328755383869875\n",
      "Initial guess B: 0.8613041593624705\n",
      "At iteration 0, theta_A = 0.649785,  theta_B = 0.759818\n",
      "At iteration 1, theta_A = 0.679508,  theta_B = 0.721210\n",
      "At iteration 2, theta_A = 0.692073,  theta_B = 0.707937\n",
      "At iteration 3, theta_A = 0.696979,  theta_B = 0.703021\n",
      "At iteration 4, theta_A = 0.698849,  theta_B = 0.701151\n",
      "At iteration 5, theta_A = 0.699562,  theta_B = 0.700438\n",
      "E-M converges at iteration 6\n",
      "RESULT:\n",
      "E-M: theta_A = 0.699562,  theta_B = 0.700438\n",
      "MLE with complete data: theta_A = 0.725000,  theta_B = 0.600000\n",
      " \n",
      " \n",
      "Initial guess A: 0.7611901604472725\n",
      "Initial guess B: 0.9602800675170383\n",
      "At iteration 0, theta_A = 0.693676,  theta_B = 0.789359\n",
      "At iteration 1, theta_A = 0.682323,  theta_B = 0.721772\n",
      "At iteration 2, theta_A = 0.692486,  theta_B = 0.707546\n",
      "At iteration 3, theta_A = 0.697132,  theta_B = 0.702868\n",
      "At iteration 4, theta_A = 0.698908,  theta_B = 0.701092\n",
      "At iteration 5, theta_A = 0.699584,  theta_B = 0.700416\n",
      "E-M converges at iteration 6\n",
      "RESULT:\n",
      "E-M: theta_A = 0.699584,  theta_B = 0.700416\n",
      "MLE with complete data: theta_A = 0.725000,  theta_B = 0.600000\n",
      " \n",
      " \n",
      "Initial guess A: 0.14163552185508943\n",
      "Initial guess B: 0.681374385001259\n",
      "At iteration 0, theta_A = 0.604852,  theta_B = 0.700169\n",
      "At iteration 1, theta_A = 0.681784,  theta_B = 0.715072\n",
      "At iteration 2, theta_A = 0.693680,  theta_B = 0.706307\n",
      "At iteration 3, theta_A = 0.697595,  theta_B = 0.702405\n",
      "At iteration 4, theta_A = 0.699084,  theta_B = 0.700916\n",
      "At iteration 5, theta_A = 0.699651,  theta_B = 0.700349\n",
      "E-M converges at iteration 6\n",
      "RESULT:\n",
      "E-M: theta_A = 0.699651,  theta_B = 0.700349\n",
      "MLE with complete data: theta_A = 0.725000,  theta_B = 0.600000\n",
      " \n",
      " \n",
      "Initial guess A: 0.15418525514259118\n",
      "Initial guess B: 0.2607121367209969\n",
      "At iteration 0, theta_A = 0.653588,  theta_B = 0.702050\n",
      "At iteration 1, theta_A = 0.690934,  theta_B = 0.708640\n",
      "At iteration 2, theta_A = 0.696630,  theta_B = 0.703370\n",
      "At iteration 3, theta_A = 0.698716,  theta_B = 0.701284\n",
      "At iteration 4, theta_A = 0.699511,  theta_B = 0.700489\n",
      "E-M converges at iteration 5\n",
      "RESULT:\n",
      "E-M: theta_A = 0.699511,  theta_B = 0.700489\n",
      "MLE with complete data: theta_A = 0.725000,  theta_B = 0.600000\n",
      " \n",
      " \n",
      "Initial guess A: 0.291793669062145\n",
      "Initial guess B: 0.8074532471043976\n",
      "At iteration 0, theta_A = 0.608662,  theta_B = 0.713060\n",
      "At iteration 1, theta_A = 0.680038,  theta_B = 0.716846\n",
      "At iteration 2, theta_A = 0.693013,  theta_B = 0.706971\n",
      "At iteration 3, theta_A = 0.697342,  theta_B = 0.702658\n",
      "At iteration 4, theta_A = 0.698987,  theta_B = 0.701013\n",
      "At iteration 5, theta_A = 0.699614,  theta_B = 0.700386\n",
      "E-M converges at iteration 6\n",
      "RESULT:\n",
      "E-M: theta_A = 0.699614,  theta_B = 0.700386\n",
      "MLE with complete data: theta_A = 0.725000,  theta_B = 0.600000\n",
      " \n",
      " \n",
      "Initial guess A: 0.15513090560989262\n",
      "Initial guess B: 0.5501919658800833\n",
      "At iteration 0, theta_A = 0.611001,  theta_B = 0.700244\n",
      "At iteration 1, theta_A = 0.683023,  theta_B = 0.714363\n",
      "At iteration 2, theta_A = 0.694047,  theta_B = 0.705943\n",
      "At iteration 3, theta_A = 0.697735,  theta_B = 0.702265\n",
      "At iteration 4, theta_A = 0.699137,  theta_B = 0.700863\n",
      "At iteration 5, theta_A = 0.699671,  theta_B = 0.700329\n",
      "E-M converges at iteration 6\n",
      "RESULT:\n",
      "E-M: theta_A = 0.699671,  theta_B = 0.700329\n",
      "MLE with complete data: theta_A = 0.725000,  theta_B = 0.600000\n",
      " \n",
      " \n",
      "Initial guess A: 0.9773436474411609\n",
      "Initial guess B: 0.8774484376180965\n",
      "At iteration 0, theta_A = 0.786770,  theta_B = 0.696992\n",
      "At iteration 1, theta_A = 0.720375,  theta_B = 0.683271\n",
      "At iteration 2, theta_A = 0.707093,  theta_B = 0.692932\n",
      "At iteration 3, theta_A = 0.702697,  theta_B = 0.697303\n",
      "At iteration 4, theta_A = 0.701027,  theta_B = 0.698973\n",
      "At iteration 5, theta_A = 0.700391,  theta_B = 0.699609\n",
      "E-M converges at iteration 6\n",
      "RESULT:\n",
      "E-M: theta_A = 0.700391,  theta_B = 0.699609\n",
      "MLE with complete data: theta_A = 0.725000,  theta_B = 0.600000\n",
      " \n",
      " \n",
      "Initial guess A: 0.9847255298923027\n",
      "Initial guess B: 0.17230824063481887\n",
      "At iteration 0, theta_A = 0.777649,  theta_B = 0.618617\n",
      "At iteration 1, theta_A = 0.729529,  theta_B = 0.670918\n",
      "At iteration 2, theta_A = 0.711135,  theta_B = 0.688889\n",
      "At iteration 3, theta_A = 0.704235,  theta_B = 0.695765\n",
      "At iteration 4, theta_A = 0.701613,  theta_B = 0.698387\n",
      "At iteration 5, theta_A = 0.700615,  theta_B = 0.699385\n",
      "E-M converges at iteration 6\n",
      "RESULT:\n",
      "E-M: theta_A = 0.700615,  theta_B = 0.699385\n",
      "MLE with complete data: theta_A = 0.725000,  theta_B = 0.600000\n",
      " \n",
      " \n",
      "Initial guess A: 0.8268636538745348\n",
      "Initial guess B: 0.5465108609850157\n",
      "At iteration 0, theta_A = 0.747591,  theta_B = 0.652070\n",
      "At iteration 1, theta_A = 0.718044,  theta_B = 0.682058\n",
      "At iteration 2, theta_A = 0.706846,  theta_B = 0.693157\n",
      "At iteration 3, theta_A = 0.702607,  theta_B = 0.697393\n",
      "At iteration 4, theta_A = 0.700993,  theta_B = 0.699007\n",
      "At iteration 5, theta_A = 0.700378,  theta_B = 0.699622\n",
      "E-M converges at iteration 6\n",
      "RESULT:\n",
      "E-M: theta_A = 0.700378,  theta_B = 0.699622\n",
      "MLE with complete data: theta_A = 0.725000,  theta_B = 0.600000\n",
      " \n",
      " \n",
      "Initial guess A: 0.8453802589933055\n",
      "Initial guess B: 0.9064041891152532\n",
      "At iteration 0, theta_A = 0.687785,  theta_B = 0.731892\n",
      "At iteration 1, theta_A = 0.691547,  theta_B = 0.708638\n",
      "At iteration 2, theta_A = 0.696745,  theta_B = 0.703255\n",
      "At iteration 3, theta_A = 0.698760,  theta_B = 0.701240\n",
      "At iteration 4, theta_A = 0.699528,  theta_B = 0.700472\n",
      "E-M converges at iteration 5\n",
      "RESULT:\n",
      "E-M: theta_A = 0.699528,  theta_B = 0.700472\n",
      "MLE with complete data: theta_A = 0.725000,  theta_B = 0.600000\n",
      " \n",
      " \n",
      "Initial guess A: 0.4895183090570616\n",
      "Initial guess B: 0.026766196979888024\n",
      "At iteration 0, theta_A = 0.700000,  theta_B = 0.601575\n",
      "At iteration 1, theta_A = 0.715413,  theta_B = 0.681141\n",
      "At iteration 2, theta_A = 0.706490,  theta_B = 0.693494\n",
      "At iteration 3, theta_A = 0.702475,  theta_B = 0.697525\n",
      "At iteration 4, theta_A = 0.700943,  theta_B = 0.699057\n",
      "At iteration 5, theta_A = 0.700359,  theta_B = 0.699641\n",
      "E-M converges at iteration 6\n",
      "RESULT:\n",
      "E-M: theta_A = 0.700359,  theta_B = 0.699641\n",
      "MLE with complete data: theta_A = 0.725000,  theta_B = 0.600000\n",
      " \n",
      " \n",
      "Initial guess A: 0.0383236805480599\n",
      "Initial guess B: 0.8520807177021352\n",
      "At iteration 0, theta_A = 0.600354,  theta_B = 0.700001\n",
      "At iteration 1, theta_A = 0.680887,  theta_B = 0.715547\n",
      "At iteration 2, theta_A = 0.693421,  theta_B = 0.706562\n",
      "At iteration 3, theta_A = 0.697498,  theta_B = 0.702502\n",
      "At iteration 4, theta_A = 0.699047,  theta_B = 0.700953\n",
      "At iteration 5, theta_A = 0.699637,  theta_B = 0.700363\n",
      "E-M converges at iteration 6\n",
      "RESULT:\n",
      "E-M: theta_A = 0.699637,  theta_B = 0.700363\n",
      "MLE with complete data: theta_A = 0.725000,  theta_B = 0.600000\n",
      " \n",
      " \n",
      "Initial guess A: 0.21348494568233423\n",
      "Initial guess B: 0.7535089375633657\n",
      "At iteration 0, theta_A = 0.605988,  theta_B = 0.702063\n",
      "At iteration 1, theta_A = 0.681642,  theta_B = 0.715259\n",
      "At iteration 2, theta_A = 0.693617,  theta_B = 0.706369\n",
      "At iteration 3, theta_A = 0.697572,  theta_B = 0.702428\n",
      "At iteration 4, theta_A = 0.699075,  theta_B = 0.700925\n",
      "At iteration 5, theta_A = 0.699648,  theta_B = 0.700352\n",
      "E-M converges at iteration 6\n",
      "RESULT:\n",
      "E-M: theta_A = 0.699648,  theta_B = 0.700352\n",
      "MLE with complete data: theta_A = 0.725000,  theta_B = 0.600000\n",
      " \n",
      " \n",
      "Initial guess A: 0.21131306880444356\n",
      "Initial guess B: 0.8610382327700159\n",
      "At iteration 0, theta_A = 0.602999,  theta_B = 0.707982\n",
      "At iteration 1, theta_A = 0.679861,  theta_B = 0.716593\n",
      "At iteration 2, theta_A = 0.693029,  theta_B = 0.706953\n",
      "At iteration 3, theta_A = 0.697349,  theta_B = 0.702652\n",
      "At iteration 4, theta_A = 0.698990,  theta_B = 0.701010\n",
      "At iteration 5, theta_A = 0.699615,  theta_B = 0.700385\n",
      "E-M converges at iteration 6\n",
      "RESULT:\n",
      "E-M: theta_A = 0.699615,  theta_B = 0.700385\n",
      "MLE with complete data: theta_A = 0.725000,  theta_B = 0.600000\n",
      " \n",
      " \n",
      "Initial guess A: 0.39719335083703444\n",
      "Initial guess B: 0.9457029427960507\n",
      "At iteration 0, theta_A = 0.640211,  theta_B = 0.785954\n",
      "At iteration 1, theta_A = 0.673766,  theta_B = 0.729299\n",
      "At iteration 2, theta_A = 0.689458,  theta_B = 0.710598\n",
      "At iteration 3, theta_A = 0.695976,  theta_B = 0.704025\n",
      "At iteration 4, theta_A = 0.698467,  theta_B = 0.701533\n",
      "At iteration 5, theta_A = 0.699416,  theta_B = 0.700584\n",
      "E-M converges at iteration 6\n",
      "RESULT:\n",
      "E-M: theta_A = 0.699416,  theta_B = 0.700584\n",
      "MLE with complete data: theta_A = 0.725000,  theta_B = 0.600000\n",
      " \n",
      " \n",
      "Initial guess A: 0.8312291428915874\n",
      "Initial guess B: 0.002936253184108506\n",
      "At iteration 0, theta_A = 0.700000,  theta_B = 0.600030\n",
      "At iteration 1, theta_A = 0.715582,  theta_B = 0.680820\n",
      "At iteration 2, theta_A = 0.706581,  theta_B = 0.693402\n",
      "At iteration 3, theta_A = 0.702510,  theta_B = 0.697490\n",
      "At iteration 4, theta_A = 0.700956,  theta_B = 0.699044\n",
      "At iteration 5, theta_A = 0.700364,  theta_B = 0.699636\n",
      "E-M converges at iteration 6\n",
      "RESULT:\n",
      "E-M: theta_A = 0.700364,  theta_B = 0.699636\n",
      "MLE with complete data: theta_A = 0.725000,  theta_B = 0.600000\n",
      " \n",
      " \n",
      "Initial guess A: 0.5644711354703983\n",
      "Initial guess B: 0.8226991437332766\n",
      "At iteration 0, theta_A = 0.655934,  theta_B = 0.746058\n",
      "At iteration 1, theta_A = 0.683048,  theta_B = 0.717116\n",
      "At iteration 2, theta_A = 0.693520,  theta_B = 0.706483\n",
      "At iteration 3, theta_A = 0.697531,  theta_B = 0.702469\n",
      "At iteration 4, theta_A = 0.699060,  theta_B = 0.700940\n",
      "At iteration 5, theta_A = 0.699642,  theta_B = 0.700358\n",
      "E-M converges at iteration 6\n",
      "RESULT:\n",
      "E-M: theta_A = 0.699642,  theta_B = 0.700358\n",
      "MLE with complete data: theta_A = 0.725000,  theta_B = 0.600000\n",
      " \n",
      " \n",
      "Initial guess A: 0.3498806748108789\n",
      "Initial guess B: 0.5910832911512018\n",
      "At iteration 0, theta_A = 0.640112,  theta_B = 0.708126\n",
      "At iteration 1, theta_A = 0.687207,  theta_B = 0.711845\n",
      "At iteration 2, theta_A = 0.695314,  theta_B = 0.704684\n",
      "At iteration 3, theta_A = 0.698215,  theta_B = 0.701785\n",
      "At iteration 4, theta_A = 0.699320,  theta_B = 0.700680\n",
      "At iteration 5, theta_A = 0.699741,  theta_B = 0.700259\n",
      "E-M converges at iteration 6\n",
      "RESULT:\n",
      "E-M: theta_A = 0.699741,  theta_B = 0.700259\n",
      "MLE with complete data: theta_A = 0.725000,  theta_B = 0.600000\n",
      " \n",
      " \n",
      "Initial guess A: 0.774397174114881\n",
      "Initial guess B: 0.6747911155705526\n",
      "At iteration 0, theta_A = 0.720966,  theta_B = 0.681531\n",
      "At iteration 1, theta_A = 0.707526,  theta_B = 0.692495\n",
      "At iteration 2, theta_A = 0.702862,  theta_B = 0.697138\n",
      "At iteration 3, theta_A = 0.701090,  theta_B = 0.698910\n",
      "At iteration 4, theta_A = 0.700415,  theta_B = 0.699585\n",
      "E-M converges at iteration 5\n",
      "RESULT:\n",
      "E-M: theta_A = 0.700415,  theta_B = 0.699585\n",
      "MLE with complete data: theta_A = 0.725000,  theta_B = 0.600000\n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from scipy.special import comb\n",
    "import math\n",
    "\n",
    "\n",
    "def compute_likelihood(obs, n, pheads): # No surprise, it's Binomial!!!\n",
    "\n",
    "    likelihood = comb(n, obs, exact=True)*(pheads**obs)*(1.0-pheads)**(n-obs)\n",
    "\n",
    "    return likelihood\n",
    "\n",
    "# generate experiments\n",
    "num_coin_toss = 10 # each experiment contains num_coin_toss tosses\n",
    "num_exp = 5  # we perform 5 such experiments\n",
    "theta_A_true = 0.8 \n",
    "theta_B_true = 0.4\n",
    "coin_choice = np.zeros(num_exp) # initialize: 0 for A and 1 for B\n",
    "head_counts = np.zeros(num_exp)\n",
    "\n",
    "MLE_A = 0\n",
    "MLE_B = 0\n",
    "    # generate the outcomes of experiment\n",
    "for i in np.arange(num_exp):\n",
    "\n",
    "    if np.random.randint(2) == 0: # coin A is selected\n",
    "        head_counts[i] = np.random.binomial(num_coin_toss , theta_A_true, 1) # toss coin A num_coin_toss times\n",
    "        MLE_A = MLE_A +  head_counts[i] # add the number of heads observed to total headcounts \n",
    "\n",
    "    else: # coin B is selected \n",
    "        head_counts[i] = np.random.binomial(num_coin_toss , theta_B_true, 1) # toss coin B num_coin_toss times\n",
    "        coin_choice[i] = 1  # record the selection of coin B during experiment i \n",
    "        MLE_B = MLE_B +  head_counts[i] # add the number of heads observed to total headcounts \n",
    "\n",
    "tail_counts = num_coin_toss - head_counts\n",
    "    # MLE \n",
    "\n",
    "    \n",
    "\n",
    "# MLE is merely the proportion of heads for each coin toss\n",
    "MLE_A = MLE_A / ((num_exp - np.count_nonzero(coin_choice))*num_coin_toss)\n",
    "MLE_B = MLE_B / (np.count_nonzero(coin_choice)*num_coin_toss)\n",
    "\n",
    "\n",
    "for i in range(20):\n",
    "    # initialize the pA(heads) and pB(heads), namely, coin biases\n",
    "    pA_heads = np.zeros(100); \n",
    "    pB_heads = np.zeros(100); \n",
    "\n",
    "    pA_heads[0] = random.uniform(0,1) # initial guess\n",
    "    pB_heads[0] = random.uniform(0,1) # initial guess\n",
    "    print(\"Initial guess A: \" + str(pA_heads[0]))\n",
    "    print(\"Initial guess B: \" + str(pB_heads[0]))\n",
    "\n",
    "    # E-M begins!\n",
    "    epsilon = 0.001   # error threshold\n",
    "    j = 0 # iteration counter\n",
    "    improvement = float('inf')\n",
    "\n",
    "    while (improvement > epsilon):\n",
    "\n",
    "        expectation_A = np.zeros((num_exp,2), dtype=float) \n",
    "        expectation_B = np.zeros((num_exp,2), dtype=float)\n",
    "\n",
    "        for i in np.arange(min(len(head_counts),len(tail_counts))):\n",
    "\n",
    "            eH = head_counts[i]\n",
    "            eT = tail_counts[i]\n",
    "\n",
    "            # E step:\n",
    "            lA = compute_likelihood(eH, num_coin_toss, pA_heads[j])\n",
    "            lB = compute_likelihood(eH, num_coin_toss, pB_heads[j])\n",
    "\n",
    "            weightA = lA / (lA + lB)\n",
    "            weightB = lB / (lA + lB)\n",
    "\n",
    "            expectation_A[i] = weightA*np.array([eH, eT])\n",
    "            expectation_B[i] = weightB*np.array([eH, eT])\n",
    "\n",
    "\n",
    "        # M step\n",
    "        theta_A = np.sum(expectation_A, axis = 0)[0] / np.sum(expectation_A) \n",
    "        theta_B = np.sum(expectation_B, axis = 0)[0] / np.sum(expectation_B) \n",
    "\n",
    "        print('At iteration %d, theta_A = %2f,  theta_B = %2f' % (j, theta_A, theta_B))\n",
    "\n",
    "        pA_heads[j+1] = sum(expectation_A)[0] / sum(sum(expectation_A)); \n",
    "        pB_heads[j+1] = sum(expectation_B)[0] / sum(sum(expectation_B)); \n",
    "\n",
    "        improvement = max( abs(np.array([pA_heads[j+1],pB_heads[j+1]]) - np.array([pA_heads[j],pB_heads[j]]) ))\n",
    "        j = j+1\n",
    "\n",
    "    # END of E-M, print the outcome\n",
    "\n",
    "    print('E-M converges at iteration %d' %j)\n",
    "    print('RESULT:')\n",
    "    print('E-M: theta_A = %2f,  theta_B = %2f' % (theta_A, theta_B))\n",
    "    print('MLE with complete data: theta_A = %2f,  theta_B = %2f' % (MLE_A, MLE_B))\n",
    "    print(' ')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 2\n",
    "\n",
    " * How fast does EM converge? Is the converged result close to what you'd get from MLE? \n",
    " \n",
    " > It converges fairly quickly (almost after the first step). Its in the ballpark of the MLE values, but is not quite the same\n",
    " \n",
    " * Following Exercise 1, what's the objective function we're optimizing in the E-step? Does this function have a *unique global maximum*? \n",
    " \n",
    " > We're maximizingthe variational free energy.  There is a global maximumcorresponding to the true values of $\\theta_A$ and $\\theta_B$\n",
    " \n",
    " * Compare both the results of MLE and EM to the actual bias (i.e. *theta_A_true*  and *theta_B_true* in the snippet above), comment on their performance.\n",
    " \n",
    " > Both are less than the true values (0.8 and 0.4 respectively), but MLE is noticably better for both parameters.  I imagine both would improve if more data was provided.  For 50 flips, statistical fluctuations are large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final remarks: a few practical tricks\n",
    "\n",
    "From Exercise 2 and Section XIV, we know that the E-M algorithm often approximates the MLE even in the presence of latent (hidden variables). Like with most optimization methods for nonconcave functions, E-M only guarantees convergence to a local maximum of the objective function. For this reason, its performance can be boosted by running the EM procedure starting with multiple initial parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "* Now instead of having a fixed initial guess of coin biases (i.e. *pA_heads[0]* and *pB_heads[0]* in the snippet), draw these values uniformly at random from $[0,1]$ and run the E-M algorithm. Repeat this twenty times and report what you observed. What's the best initial guess that gives the closest estimate to the true parameters?\n",
    "\n",
    "> The labels, A/B for the coins are arbitrary, and EM is indifferent to what they are called, so I found that which ever initial value was higher would approach the greater of the 2 truth values, which was interesting to note.\n",
    "> I did find that the initial values of of the parameters were not very influential on *where* EM converged; only how fast.  All 20 samples ended with roughly the same 2 EM values.  How close that was to the MLE was a function of the how the coins flipped in the sample generation.\n",
    "> Generally, it seems that EM has an easier time approaching the MLE values when the sameples drawn are \"extreme,\" meaning >70% heads, or <30% heads.\n",
    "\n",
    "* As we discussed in Section X (LinReg), **Maximum a posteriori (MAP)** estimation differs from MLE in that it employs an augmented objective function which incorporates a prior distribution over the quantities we want to estimate, and the prior distribution can be think of as a regularizer for the objective fuction used in MLE. Here we will explore how to extend E-M to MAP estimation. \n",
    "\n",
    "  (1) First derive the MAP estimate for the one-coin-flipping example, namely,\n",
    "  $$\n",
    "  \\hat{{\\theta}}_{MAP}(\\boldsymbol{x}) = \\arg\\max_{\\theta\\in[0,1]} \\log P(\\boldsymbol{x}|n,{\\theta} ) + \\log P({\\theta}),\n",
    "  $$\n",
    "  where \n",
    "  $$P(\\boldsymbol{x}|n,{\\theta}) = \\prod_{i=1}^{10} \\text{Binomial} (x_i|n,\\theta)$$\n",
    "  \n",
    "  $$P({\\theta})=\\mathcal{N}(\\theta|\\mu, \\sigma)$$\n",
    "  \n",
    "> $$\n",
    "   0 = \\frac{d}{d \\theta}\\left[\\log P(\\boldsymbol{x}|n,{\\theta} ) + \\log P({\\theta})\\right]\n",
    "  $$  \n",
    "> $$\n",
    "   =\\frac{d}{d \\theta}\\left[\\log \\prod_{i=1}^{10} \\text{Binomial} (x_i|n,\\theta) + \\log \\mathcal{N}(\\theta|\\mu, \\sigma)\\right]\n",
    "  $$\n",
    "> Since we are taking the log of both distributions, the prefactors on each of them become seperate logs terms, the derivatives of which are $0$. This just leaves:\n",
    "\n",
    "> $$\n",
    "   0 =\\frac{d}{d \\theta}\\sum^{10}_{i =1} \\left[ (x_i \\log{\\theta}) - (n - x_i) \\log(1- \\theta) - \\frac{(\\theta - \\mu)^2}{\\sigma^2} \\right]\n",
    "  $$\n",
    "> $$\n",
    "   0 =\\sum^{10}_{i =1} \\left( \\frac{x_i}{\\theta} - \\frac{n - x_i}{1- \\theta} \\right) - 2\\frac{(\\theta - \\mu)}{\\sigma^2}\n",
    "  $$\n",
    "> $$\n",
    "   \\sum^{10}_{i =1}\\frac{1}{\\theta (1 - \\theta)} \\left( x_i - \\theta n \\right) =   2\\frac{(\\theta - \\mu)}{\\sigma^2}\n",
    "  $$\n",
    "> The sum over $x_i$ is just the total number of head that manifested over the experiment (N):\n",
    "> $$\n",
    "  \\frac{1}{\\theta (1 - \\theta)} \\left( N - \\theta n \\right) =   2\\frac{(\\theta - \\mu)}{\\sigma^2}\n",
    "  $$\n",
    "  \n",
    "(2) Based on (1), now modify the E-M snippet above to incorporate this prior distribution into the **M-step**. Comment on the performance. For the prior choice, try $P(\\boldsymbol{\\theta})=\\mathcal{N}(\\theta_A|0.83, 1)\\mathcal{N}(\\theta_B|0.37, 1)$.\n",
    "\n",
    ">  ##################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
